# -*- coding: utf-8 -*-
"""Motus (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TyUScdUonHzcVVbgTCrdPa_BUM6rPJF0
"""

# Install dependencies
!pip install mediapipe tensorflow tensorflow-addons

"""1.Pose Detection and Video Processing
python
Copy code

"""

import cv2
import mediapipe as mp
import numpy as np

mp_pose = mp.solutions.pose
pose = mp_pose.Pose()

def calculate_angle(a, b, c):
    a = np.array(a)
    b = np.array(b)
    c = np.array(c)
    radians = np.arctan2(c[1] - b[1], c[0] - b[0]) - np.arctan2(a[1] - b[1], a[0] - b[0])
    angle = np.abs(radians * 180.0 / np.pi)
    return 360 - angle if angle > 180.0 else angle

video_path = "/content/istockphoto-925697370-640_adpp_is.mp4"
cap = cv2.VideoCapture(video_path)
angles = []

output_path = "output_video.mp4"
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
out = cv2.VideoWriter(output_path, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    frame_resized = cv2.resize(frame, (224, 224))
    image = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)
    results = pose.process(image)

    if results.pose_landmarks:
        landmarks = results.pose_landmarks.landmark
        hip = [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x, landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y]
        knee = [landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].x, landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].y]
        ankle = [landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].x, landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].y]

        angle = calculate_angle(hip, knee, ankle)
        angles.append(angle)

        frame_height, frame_width, _ = frame.shape
        hip_coords = (int(hip[0] * frame_width), int(hip[1] * frame_height))
        knee_coords = (int(knee[0] * frame_width), int(knee[1] * frame_height))
        ankle_coords = (int(ankle[0] * frame_width), int(ankle[1] * frame_height))

        cv2.line(frame, hip_coords, knee_coords, (255, 0, 0), 2)
        cv2.line(frame, knee_coords, ankle_coords, (255, 0, 0), 2)
        cv2.putText(frame, f'Angle: {int(angle)}', (knee_coords[0] - 50, knee_coords[1] - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2)

    out.write(frame)

cap.release()
out.release()
pose.close()
print("Knee Angles:", angles)

"""Mobile Optimization and TensorFlow Lite Conversion"""

import tensorflow as tf
import numpy as np

model = tf.keras.Sequential([
    tf.keras.layers.InputLayer(input_shape=(224, 224, 3)),
    tf.keras.layers.Conv2D(16, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

model.save("pose_model.h5")

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_types = [tf.float16]

converter.representative_dataset = lambda: [
    [np.random.rand(1, 224, 224, 3).astype(np.float32)]
]

tflite_model = converter.convert()
with open("pose_detection_model1.tflite", "wb") as f:
    f.write(tflite_model)

print("TensorFlow Lite model saved as pose_detection_model1.tflite.")

"""TensorFlow Lite Inference"""

import tensorflow as tf
import numpy as np

interpreter = tf.lite.Interpreter(model_path="pose_detection_model1.tflite")
interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

input_shape = input_details[0]['shape']
dummy_input = np.random.rand(*input_shape).astype(np.float32)

interpreter.set_tensor(input_details[0]['index'], dummy_input)
interpreter.invoke()

output_data = interpreter.get_tensor(output_details[0]['index'])
print("Inference output:", output_data)